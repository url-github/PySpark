{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "BiFxxg1Yh4DQ"
      },
      "id": "BiFxxg1Yh4DQ"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qwmZ_wXDh64U",
        "outputId": "3ab05044-33a7-4feb-d4ea-a77380f9add7"
      },
      "id": "qwmZ_wXDh64U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=913128bd895cc1133cd74080efc9d66b213ebdd9eb47f469b28aea0c5efccfcd\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f1bac67-57ad-41db-a1b1-4663ce41406e",
      "metadata": {
        "id": "0f1bac67-57ad-41db-a1b1-4663ce41406e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType, DateType, TimestampType\n",
        "from pyspark.sql.functions import (\n",
        "    col, size, lit, explode,\n",
        "    concat, concat_ws, substring,\n",
        "    datediff, date_add, date_sub,\n",
        "    year, month, dayofmonth, dayofweek, dayofyear, weekofyear,\n",
        "    hour, minute, second,\n",
        "    count, min, max, avg, sum, udf, when\n",
        ")\n",
        "from datetime import datetime\n",
        "\n",
        "spark = SparkSession.builder.appName(\"a\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Row\n",
        "# Importuje klasę SparkSession, która umożliwia tworzenie i zarządzanie sesją Spark.\n",
        "# Importuje klasę Row, która umożliwia tworzenie wierszy (rekordów) dla DataFrame w PySpark.\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType, DateType, TimestampType\n",
        "# Importuje typy danych (Schematy) używane do definiowania struktury kolumn w DataFrame:\n",
        "# - StructType: definiuje schemat dla DataFrame.\n",
        "# - StructField: definiuje kolumny w StructType.\n",
        "# - StringType, IntegerType: określają typ danych (np. tekst, liczba całkowita).\n",
        "# - ArrayType: typ danych dla tablicy.\n",
        "# - MapType: typ danych dla mapy (klucz-wartość).\n",
        "# - DateType, TimestampType: typy danych dla daty i znacznika czasu.\n",
        "\n",
        "from pyspark.sql.functions import (\n",
        "    col, size, lit, explode,\n",
        "    # col: pozwala odnosić się do kolumn w DataFrame.\n",
        "    # size: zwraca rozmiar (np. długość tablicy) kolumny.\n",
        "    # lit: dodaje stałą wartość do DataFrame.\n",
        "    # explode: rozwija kolumnę typu tablicowego (Array) na wiele wierszy.\n",
        "\n",
        "    concat, concat_ws, substring,\n",
        "    # concat: łączy kolumny w jedną.\n",
        "    # concat_ws: łączy kolumny z separatorem (np. przecinek).\n",
        "    # substring: wycina fragment tekstu (substring) z kolumny.\n",
        "\n",
        "    datediff, date_add, date_sub,\n",
        "    # datediff: zwraca różnicę między dwoma datami w dniach.\n",
        "    # date_add: dodaje określoną liczbę dni do daty.\n",
        "    # date_sub: odejmuje określoną liczbę dni od daty.\n",
        "\n",
        "    year, month, dayofmonth, dayofweek, dayofyear, weekofyear,\n",
        "    # year: zwraca rok z daty.\n",
        "    # month: zwraca miesiąc z daty.\n",
        "    # dayofmonth: zwraca dzień miesiąca.\n",
        "    # dayofweek: zwraca dzień tygodnia (np. poniedziałek = 1).\n",
        "    # dayofyear: zwraca dzień roku.\n",
        "    # weekofyear: zwraca tydzień roku.\n",
        "\n",
        "    hour, minute, second,\n",
        "    # hour: zwraca godzinę z daty/czasu.\n",
        "    # minute: zwraca minutę z daty/czasu.\n",
        "    # second: zwraca sekundę z daty/czasu.\n",
        "\n",
        "    count, min, max, avg, sum, udf, when\n",
        "    # count: liczy ilość wierszy.\n",
        "    # min: zwraca najmniejszą wartość w kolumnie.\n",
        "    # max: zwraca największą wartość w kolumnie.\n",
        "    # avg: oblicza średnią z wartości w kolumnie.\n",
        "    # sum: sumuje wartości w kolumnie.\n",
        "    # udf: pozwala na definiowanie własnych funkcji (user-defined functions).\n",
        "    # when: wykonuje warunkowe operacje na kolumnach.\n",
        "\n",
        ")\n",
        "from datetime import datetime\n",
        "# Importuje klasę datetime z biblioteki standardowej Pythona do operacji na datach i godzinach.\n",
        "\n",
        "spark = SparkSession.builder.appName(\"a\").getOrCreate()\n",
        "# Tworzy lub uzyskuje istniejącą sesję Spark z nazwą aplikacji \"a\".\n",
        "# SparkSession to główny punkt dostępu do funkcji Spark, takich jak tworzenie DataFrame."
      ],
      "metadata": {
        "id": "-ojYngGlpSlb"
      },
      "id": "-ojYngGlpSlb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Metody na klasie SparkSession"
      ],
      "metadata": {
        "id": "6soibTIQPRCf"
      },
      "id": "6soibTIQPRCf"
    },
    {
      "cell_type": "code",
      "source": [
        "spark.createDataFrame(...)\n",
        "\n",
        "spark.read.format('csv').options(header=True, sep=\",\").load('data/best_selling_books.csv').show()\n",
        "spark.read.format(\"csv\").schema(schema).load(\"data/best_selling_books.csv\")\n",
        "spark.read.csv(\"data/Games.csv\", header=True, quote=\"\\\"\") # \"Atari, Inc. (Windows)\"\n",
        "spark.read.csv(\"data/best_selling_books.csv\", header=True, schema=schema) # W argumencie schema definiuje nowych schemat\n",
        "df.printSchema() # Wyświetlenie schematu\n",
        "\n",
        "df.select (col('name'), col('sales'), col('developer')).show()\n",
        "df.orderBy(col(\"developer\").asc(), col(\"sales\").desc()).show()\n",
        "\n",
        "df.limit(1).collect()\n",
        "df.limit(1).collect()[0][1] # 42.0\n",
        "\n",
        "df.withColumn('SalesX1000', col('sales') * 1000 ) # Dodanie nowej kolumny\n",
        "\n",
        "df.select( col(\"skills\")[1], col(\"role\")[\"level\"] ).show()\n",
        "df.select( col(\"skills\").getItem(1), col(\"role\").getItem('level') ).show()\n",
        "# size sprawdza liczbę elementów w tablicy lub słowniku\n",
        "df.select( col(\"skills\").getItem(1), col(\"role\").getItem('level'), size(col('skills')), size(col('role')) ).show()\n"
      ],
      "metadata": {
        "id": "wy6y2d4hPXs2"
      },
      "id": "wy6y2d4hPXs2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Funkcje"
      ],
      "metadata": {
        "id": "OzTGpp7Wq01O"
      },
      "id": "OzTGpp7Wq01O"
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"company\", lit('Dziurex')).show() # lit: dodaje stałą wartość do kolumny\n",
        "df.select(col('id'), explode(col('skills'))).show() # funkcja explode rozpakowuje elementy w kolumnie typu listy lub tablicy (tutaj skills)\n",
        "df.withColumn(\"employee\", concat(col('first'), lit(' '), col('last'))).show() # lit: dodaje stałą wartość do kolumny\n",
        "df.select(concat_ws(',', col('id'), col('first'), col('last'))).show() # funkcja concat_ws łączy wartości z kilku kolumn w jeden ciąg znaków (string)\n",
        "df.select(substring(col('first'), 0, 2), col('first')[0:2]).show() # substring: wycina podciąg (fragment tekstu)\n",
        "df.select(datediff(col('hire_date'), lit(datetime(2023, 9, 1))) ).show()\n",
        "df.select(col('hire_date'), date_add(col('hire_date'), 10), date_sub(col('hire_date'), 30)).show() # dodaje 10 dni / odejmuje 30 dni"
      ],
      "metadata": {
        "id": "P7i4smMJq2cE"
      },
      "id": "P7i4smMJq2cE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Klasa Row"
      ],
      "metadata": {
        "id": "A_IF0BiOpEoG"
      },
      "id": "A_IF0BiOpEoG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Tworzenie wiersza z polami nazwanymi\n",
        "row = Row(name=\"John\", age=30)\n",
        "row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY1r1GCSpEBH",
        "outputId": "318757e8-1a61-4d85-86f3-8d76d526dd08"
      },
      "id": "PY1r1GCSpEBH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(name='John', age=30)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row = [Row(name=\"John\", age=30)]\n",
        "row = spark.createDataFrame(row)\n",
        "row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u2bkwIB9E4L",
        "outputId": "e7fbf346-0c51-47a7-cb93-aeef4a2024a0"
      },
      "id": "8u2bkwIB9E4L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[name: string, age: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tworzenie DataFrame z wierszy\n",
        "data = [Row(name=\"John\", age=30), Row(name=\"Alice\", age=25)]\n",
        "df = spark.createDataFrame(data)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ninldb1lpW5w",
        "outputId": "34071452-e7d9-4696-a5cb-f7ecad8acb4b"
      },
      "id": "ninldb1lpW5w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[name: string, age: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsGxU7eYpoeo",
        "outputId": "5538ab8c-dd77-4c31-dbe9-9472751959e9"
      },
      "id": "qsGxU7eYpoeo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "| John| 30|\n",
            "|Alice| 25|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Metody PySpark"
      ],
      "metadata": {
        "id": "CoxFCzJ1--n0"
      },
      "id": "CoxFCzJ1--n0"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Metoda toDF() – konwersja RDD na DataFrame\n",
        "# Tworzy DataFrame z istniejącego RDD.\n",
        "rdd = spark.sparkContext.parallelize([(\"Alice\", 25), (\"Bob\", 30)])\n",
        "df = rdd.toDF([\"Name\", \"Age\"])\n",
        "\n",
        "# 2. Metoda show() – wyświetla dane w DataFrame\n",
        "# Wyświetla dane DataFrame w formie tabelarycznej.\n",
        "df.show()\n",
        "\n",
        "# 3. Metoda printSchema() – wyświetla schemat DataFrame\n",
        "# Wyświetla strukturę kolumn i ich typy danych.\n",
        "df.printSchema()\n",
        "\n",
        "# 4. Metoda select() – wybór kolumn\n",
        "# Wybiera określone kolumny z DataFrame.\n",
        "df.select(\"Name\").show()\n",
        "\n",
        "# 5. Metoda filter() – filtrowanie wierszy\n",
        "# Filtrowanie danych na podstawie warunków.\n",
        "df.filter(df[\"Age\"] > 25).show()\n",
        "\n",
        "# 6. Metoda groupBy() – grupowanie danych\n",
        "# Grupuje dane na podstawie określonej kolumny.\n",
        "df.groupBy(\"Age\").count().show()\n",
        "\n",
        "# 7. Metoda agg() – agregacja\n",
        "# Wykonuje różne operacje agregujące na grupach danych.\n",
        "df.groupBy(\"Age\").agg({\"Age\": \"avg\"}).show()\n",
        "\n",
        "# 8. Metoda orderBy() – sortowanie danych\n",
        "# Sortuje dane według wybranych kolumn.\n",
        "df.orderBy(df[\"Age\"].desc()).show()\n",
        "\n",
        "# 9. Metoda withColumn() – dodawanie nowej kolumny\n",
        "# Tworzy nową kolumnę w DataFrame na podstawie istniejących danych.\n",
        "df.withColumn(\"DoubleAge\", df[\"Age\"] * 2).show()\n",
        "\n",
        "# 10. Metoda drop() – usuwanie kolumn\n",
        "# Usuwa kolumnę z DataFrame.\n",
        "df.drop(\"DoubleAge\").show()\n",
        "\n",
        "# 11. Metoda distinct() – usuwanie duplikatów\n",
        "# Zwraca unikalne wiersze w DataFrame.\n",
        "df.distinct().show()\n",
        "\n",
        "# 12. Metoda count() – liczenie wierszy\n",
        "# Zwraca liczbę wierszy w DataFrame.\n",
        "row_count = df.count()\n",
        "\n",
        "# 13. Metoda union() – łączenie DataFrame\n",
        "# Łączy dwa DataFrame w jeden.\n",
        "df2 = spark.createDataFrame([(\"Charlie\", 35)], [\"Name\", \"Age\"])\n",
        "union_df = df.union(df2)\n",
        "union_df.show()\n",
        "\n",
        "# 14. Metoda join() – łączenie DataFrame na podstawie klucza\n",
        "# Łączy dwa DataFrame na podstawie wspólnej kolumny (join na kluczu).\n",
        "joined_df = df.join(df2, \"Age\", \"inner\")\n",
        "joined_df.show()\n",
        "\n",
        "# 15. Metoda describe() – statystyki opisowe\n",
        "# Zwraca statystyki opisowe dla wybranych kolumn.\n",
        "df.describe().show()\n",
        "\n",
        "# 16. Metoda write() – zapis danych do formatu plikowego\n",
        "# Zapisuje DataFrame do określonego formatu pliku.\n",
        "df.write.csv(\"path/to/output.csv\")"
      ],
      "metadata": {
        "id": "y0xDbdsC-7Aj"
      },
      "id": "y0xDbdsC-7Aj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Użycie RDD:"
      ],
      "metadata": {
        "id": "5Pj-le9RF-hP"
      },
      "id": "5Pj-le9RF-hP"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "# Wczytaj plik jako RDD\n",
        "rdd = sc.textFile(\"gs://twoje-bucket/plik.csv\")\n",
        "rdd = rdd.map(lambda line: line.split(\",\"))\n",
        "# Przetwarzanie RDD..."
      ],
      "metadata": {
        "id": "JDca3odfF_ym"
      },
      "id": "JDca3odfF_ym",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Użycie DataFrame:"
      ],
      "metadata": {
        "id": "BGjqYN7iGGZ4"
      },
      "id": "BGjqYN7iGGZ4"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
        "# Wczytaj plik jako DataFrame\n",
        "df = spark.read.csv(\"gs://twoje-bucket/plik.csv\", header=True, inferSchema=True)\n",
        "# Przetwarzanie DataFrame..."
      ],
      "metadata": {
        "id": "rtCIiPMYGG-_"
      },
      "id": "rtCIiPMYGG-_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Różne typy danych w DataFrame"
      ],
      "metadata": {
        "id": "egYp2jTOKYi_"
      },
      "id": "egYp2jTOKYi_"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, IntegerType, StringType, DoubleType,\n",
        "    BinaryType, BooleanType, DateType, TimestampType, ArrayType, MapType\n",
        ")\n",
        "\n",
        "schema = StructType(\n",
        "    [\n",
        "        StructField(\"Book\", StringType(), False),  # Przykład: \"The Hobbit\"\n",
        "        StructField(\"First published\", IntegerType(), False),  # Przykład: 1937\n",
        "        StructField(\"Sales\", DoubleType(), False),  # Przykład: 150.5 (miliony egzemplarzy)\n",
        "        StructField(\"CoverImage\", BinaryType(), True),  # Przykład: b'\\x89PNG...' (binarny zapis obrazu)\n",
        "        StructField(\"IsAvailable\", BooleanType(), True),  # Przykład: True (czy dostępna)\n",
        "        StructField(\"PublicationDate\", DateType(), True),  # Przykład: 1937-09-21 (data wydania)\n",
        "        StructField(\"LastCheckedOut\", TimestampType(), True),  # Przykład: 2023-10-08 14:45:00 (ostatnie wypożyczenie)\n",
        "        StructField(\"Tags\", ArrayType(StringType()), True),  # Przykład: [\"Fantasy\", \"Adventure\"]\n",
        "        StructField(\"ExtraInfo\", MapType(StringType(), StringType()), True),  # Przykład: {\"ISBN\": \"978-3-16-148410-0\", \"Publisher\": \"Allen & Unwin\"}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "l9R35qVqSgpM"
      },
      "id": "l9R35qVqSgpM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##UDF (User Defined Functions)"
      ],
      "metadata": {
        "id": "Vs8hu-Z2KbgB"
      },
      "id": "Vs8hu-Z2KbgB"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Definiowanie funkcji\n",
        "def to_upper(s):\n",
        "    return s.upper()\n",
        "\n",
        "# Rejestracja funkcji jako UDF\n",
        "upper_udf = udf(to_upper, StringType())\n",
        "df.withColumn(\"UpperName\", upper_udf(df[\"Name\"])).show()"
      ],
      "metadata": {
        "id": "Ou3aeSfYLTOp"
      },
      "id": "Ou3aeSfYLTOp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Okna (Window Functions)"
      ],
      "metadata": {
        "id": "yrjEXR5tKmtQ"
      },
      "id": "yrjEXR5tKmtQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "window_spec = Window.orderBy(\"Age\")\n",
        "df.withColumn(\"Rank\", rank().over(window_spec)).show()"
      ],
      "metadata": {
        "id": "nkN0F-VlLQxK"
      },
      "id": "nkN0F-VlLQxK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Przetwarzanie strumieniowe (Structured Streaming)"
      ],
      "metadata": {
        "id": "hjYhVHztKs6B"
      },
      "id": "hjYhVHztKs6B"
    },
    {
      "cell_type": "code",
      "source": [
        "# Przykład stworzenia DataFrame ze strumienia\n",
        "df_streaming = spark.readStream.format(\"csv\").option(\"header\", \"true\").load(\"path/to/streaming/files\")"
      ],
      "metadata": {
        "id": "Xtzdo06lLOMh"
      },
      "id": "Xtzdo06lLOMh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Zapis i odczyt danych"
      ],
      "metadata": {
        "id": "s_g9gn9uLEAT"
      },
      "id": "s_g9gn9uLEAT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Zapis do formatu Parquet\n",
        "df.write.parquet(\"path/to/output.parquet\")"
      ],
      "metadata": {
        "id": "KIBUmUYKLLav"
      },
      "id": "KIBUmUYKLLav",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Wbudowane funkcje analizy"
      ],
      "metadata": {
        "id": "-nG1tRLyLHwL"
      },
      "id": "-nG1tRLyLHwL"
    },
    {
      "cell_type": "code",
      "source": [
        "df.stat.corr(\"Age\", \"Income\")"
      ],
      "metadata": {
        "id": "lVLrREd4KZPk"
      },
      "id": "lVLrREd4KZPk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Integracja z MLlib"
      ],
      "metadata": {
        "id": "Sp8sSFtvLX71"
      },
      "id": "Sp8sSFtvLX71"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "model = lr.fit(trainingData)"
      ],
      "metadata": {
        "id": "EgzR-FYMLYlT"
      },
      "id": "EgzR-FYMLYlT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}